# LLM Service Configuration
# Copy this file to .env and fill in your API keys

# Google Gemini AI
# Get your API key from: https://makersuite.google.com/app/apikey
GOOGLE_GEMINI_API_KEY=your-gemini-api-key-here

# OpenAI
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-api-key-here

# Anthropic Claude
# Get your API key from: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here

# HuggingFace
# Get your API token from: https://huggingface.co/settings/tokens
HUGGINGFACE_API_TOKEN=hf_your-huggingface-token-here

# Mistral AI
# Get your API key from: https://console.mistral.ai/api-keys
MISTRAL_API_KEY=your-mistral-api-key-here

# Ollama Configuration (Local)
# No API key needed - runs locally
OLLAMA_ENDPOINT=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b

# Default Provider
# Options: ollama, openai, anthropic, gemini, huggingface, mistral
DEFAULT_LLM_PROVIDER=ollama

# Cache Configuration
LLM_CACHE_ENABLED=true
LLM_CACHE_TTL=60000
LLM_CACHE_MAX_SIZE=1048576

# Rate Limiting
LLM_RATE_LIMIT_REQUESTS_PER_MINUTE=60
LLM_RATE_LIMIT_TOKENS_PER_MINUTE=100000